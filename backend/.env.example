# Environment Configuration for EIDO Backend

# Application
DEBUG=true
ENVIRONMENT=development
LOG_LEVEL=INFO

# Database
DATABASE_URL=sqlite:///./eido.db

# CORS
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8000

# ─── AI Models ──────────────────────────────────────────────
# Default: Groq Llama 3.3 70B (Free, fast, high quality)
# Override per-stage if needed
DEFAULT_LLM_MODEL=llama-3.3-70b-versatile
IDEATION_LLM_MODEL=llama-3.3-70b-versatile
ARCHITECTURE_LLM_MODEL=llama-3.3-70b-versatile
BUILDING_LLM_MODEL=llama-3.3-70b-versatile
DEPLOYMENT_LLM_MODEL=llama-3.3-70b-versatile
TOKENIZATION_LLM_MODEL=llama-3.3-70b-versatile
SUMMARY_LLM_MODEL=llama-3.3-70b-versatile

# ─── AI Provider Keys ──────────────────────────────────────
# GROQ (Free & Fast - RECOMMENDED): https://console.groq.com/
GROQ_API_KEY=your_groq_api_key_here
# GEMINI (Free Tier Available): https://aistudio.google.com/
GEMINI_API_KEY=
# OPENAI (Paid API): https://platform.openai.com/
OPENAI_API_KEY=
# ANTHROPIC (Paid API): https://console.anthropic.com/
ANTHROPIC_API_KEY=
# OLLAMA (Local, Free): https://ollama.com/
# To use: set DEFAULT_LLM_MODEL=ollama/llama3 (or any model you've pulled)
OLLAMA_BASE_URL=http://localhost:11434/v1

# ─── External Services ─────────────────────────────────────
# SURGE (Tokenization)
SURGE_API_KEY=your_surge_api_key_here
SURGE_TESTNET=true
# Moltbook (Publishing)
MOLTBOOK_API_KEY=your_moltbook_api_key_here
# here.now (Deployment)
HERENOW_API_KEY=your_herenow_api_key_here

# ─── Agent Configuration ───────────────────────────────────
MAX_AGENT_RETRIES=3
AGENT_TIMEOUT_SECONDS=300
MAX_STAGE_RETRIES=2

# ─── AI Runtime Limits ─────────────────────────────────────
MAX_TOTAL_COST=10.0
MAX_TOTAL_RUNTIME=3600

# ─── Rate Limiting ─────────────────────────────────────────
RATE_LIMIT_ENABLED=true
RATE_LIMIT_STORAGE=memory  # memory or redis
REDIS_URL=redis://localhost:6379/0

# Rate limits (format: count/period where period is second, minute, hour, or day)
MVP_CREATION_LIMIT=10/hour
MVP_LIST_LIMIT=100/minute
MVP_GET_LIMIT=200/minute
GLOBAL_API_LIMIT=1000/minute

# Concurrent pipeline limits
MAX_CONCURRENT_PIPELINES_PER_USER=3
MAX_CONCURRENT_PIPELINES_GLOBAL=50

# ─── Metrics & Monitoring ──────────────────────────────────
METRICS_ENABLED=true
METRICS_PORT=9090
METRICS_PATH=/metrics

# Health checks
HEALTH_CHECK_ENABLED=true
HEALTH_CHECK_DEEP=false

# ─── Alerting ──────────────────────────────────────────────
ALERT_COST_THRESHOLD=100.0  # USD per day
ALERT_ERROR_RATE_THRESHOLD=0.1  # 10%
ALERT_WEBHOOK_URL=  # Slack or Discord webhook URL
